{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load data and folds] done in 1 s (Total: 0.78 sec)\n",
      "[preprocessing] done in 0 s (Total: 0.99 sec)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "\n",
    "# from utils import Timer, seed_everything\n",
    "# from utils import DataLoader, Yml, make_submission\n",
    "# from utils import send_line, send_notion\n",
    "# from runner import train_and_predict, save_importances, save_oof_plot, save_learning_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import requests\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# ===============\n",
    "# Utils\n",
    "# ===============\n",
    "class Timer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.processing_time = 0\n",
    "\n",
    "    @contextmanager\n",
    "    def timer(self, name):\n",
    "        t0 = time.time()\n",
    "        yield\n",
    "        t1 = time.time()\n",
    "        processing_time = t1 - t0\n",
    "        self.processing_time += round(processing_time, 2)\n",
    "        if self.processing_time < 60:\n",
    "            print(f'[{name}] done in {processing_time:.0f} s (Total: {self.processing_time:.2f} sec)')\n",
    "        elif self.processing_time < 3600:\n",
    "            print(f'[{name}] done in {processing_time:.0f} s (Total: {self.processing_time / 60:.2f} min)')\n",
    "        else:\n",
    "            print(f'[{name}] done in {processing_time:.0f} s (Total: {self.processing_time / 3600:.2f} hour)')\n",
    "\n",
    "    def get_processing_time(self):\n",
    "        return round(self.processing_time, 2)\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def load_x(self, features, data_type='train', reduce=False):\n",
    "        dfs = [pd.read_feather(f'../features/{f}_{data_type}.feather') for f in features]\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "\n",
    "        if reduce:\n",
    "            df = self.reduce_mem_usage(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_y(self, train_y_path):\n",
    "        train_y = joblib.load(train_y_path)\n",
    "        return train_y\n",
    "\n",
    "    def load_folds(self, fold_name):\n",
    "        folds = pd.read_feather(f'../folds/{fold_name}.feather')\n",
    "        return folds\n",
    "\n",
    "    def reduce_mem_usage(self, df):\n",
    "        start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        # print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "        # print(\"column = \", len(df.columns))\n",
    "        for i, col in enumerate(df.columns):\n",
    "            try:\n",
    "                col_type = df[col].dtype\n",
    "\n",
    "                if col_type != object:\n",
    "                    c_min = df[col].min()\n",
    "                    c_max = df[col].max()\n",
    "                    if str(col_type)[:3] == 'int':\n",
    "                        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                            df[col] = df[col].astype(np.int8)\n",
    "                        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                            df[col] = df[col].astype(np.int16)\n",
    "                        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                            df[col] = df[col].astype(np.int32)\n",
    "                        elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                            df[col] = df[col].astype(np.int32)\n",
    "                    else:\n",
    "                        if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                            df[col] = df[col].astype(np.float32)\n",
    "                        elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                            df[col] = df[col].astype(np.float32)\n",
    "                        else:\n",
    "                            df[col] = df[col].astype(np.float32)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "        print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "def make_submission(y_pred, target_name, sample_path, output_path, comp=False):\n",
    "    df_sub = pd.read_feather(sample_path)\n",
    "    df_sub[target_name] = y_pred\n",
    "    if comp:\n",
    "        output_path += '.gz'\n",
    "        df_sub.to_csv(output_path, index=False, compression='gzip')\n",
    "    else:\n",
    "        df_sub.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "class Yml:\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(path, 'r') as yf:\n",
    "            yaml_file = yaml.load(yf)\n",
    "        return yaml_file\n",
    "\n",
    "    def save(self, path, data):\n",
    "        with open(path, 'w') as yf:\n",
    "            yf.write(yaml.dump(data, default_flow_style=False))\n",
    "\n",
    "\n",
    "# ===============\n",
    "# Settings\n",
    "# ===============\n",
    "yml = Yml()\n",
    "config = yml.load('../configs/common/default.yml')\n",
    "config.update(yml.load(f'../configs/exp/nn_reg01.yml'))\n",
    "\n",
    "# ===============\n",
    "# Constants\n",
    "# ===============\n",
    "NOW = datetime.datetime.now()\n",
    "MODEL_NAME = 'nn_reg01'\n",
    "RUN_NAME = f'{MODEL_NAME}_{NOW:%Y%m%d%H%M%S}'\n",
    "\n",
    "COMPE_PARAMS = config['compe']\n",
    "SETTINGS_PARAMS = config['settings']\n",
    "PATH_PARAMS = config['path']\n",
    "MODEL_PARAMS = config['model_params']\n",
    "\n",
    "FEATURES = SETTINGS_PARAMS['features']\n",
    "\n",
    "\n",
    "# ===============\n",
    "# Main\n",
    "# ===============\n",
    "t = Timer()\n",
    "seed_everything(COMPE_PARAMS['seed'])\n",
    "\n",
    "with t.timer('load data and folds'):\n",
    "    loader = DataLoader()\n",
    "    train_x = loader.load_x(FEATURES, data_type='train', reduce=SETTINGS_PARAMS['reduce'])\n",
    "    test_x = loader.load_x(FEATURES, data_type='test', reduce=SETTINGS_PARAMS['reduce'])\n",
    "    train_y = loader.load_y(PATH_PARAMS['train_y'])\n",
    "    folds = loader.load_folds(SETTINGS_PARAMS['fold_name'])\n",
    "\n",
    "with t.timer('preprocessing'):\n",
    "    if SETTINGS_PARAMS['drop_fname'] is not None:\n",
    "        drop_idx = np.load(f'../pickle/{SETTINGS_PARAMS[\"drop_fname\"]}')\n",
    "        train_x = train_x.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "        train_y = train_y.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "        folds = folds.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "\n",
    "    if SETTINGS_PARAMS['oof']['add'] is not None:\n",
    "        OOF_RUN_NAME = SETTINGS_PARAMS['oof']['add']\n",
    "        oof = np.load(f'../logs/{OOF_RUN_NAME}/oof.npy')\n",
    "        pred = pd.read_csv(f'../data/output/{OOF_RUN_NAME}.csv')[COMPE_PARAMS['target_name']].values\n",
    "        train_x['oof'] = oof\n",
    "        test_x['oof'] = pred\n",
    "        FEATURES += ['oof']\n",
    "\n",
    "    if SETTINGS_PARAMS['std']:\n",
    "        whole = pd.concat([train_x, test_x], axis=0)\n",
    "        len_train = len(train_x)\n",
    "        scaler = StandardScaler()\n",
    "        whole = pd.DataFrame(scaler.fit_transform(whole), columns=whole.columns)\n",
    "        train_x = whole.iloc[:len_train]\n",
    "        test_x = whole.iloc[len_train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from functools import partial\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "class Scorer:\n",
    "\n",
    "    def scorer(self, metrics, y_true, y_pred):\n",
    "        if metrics == 'rmse':\n",
    "            return self._rmse(y_true, y_pred)\n",
    "        elif metrics == 'rmsle':\n",
    "            return self._rmsle(y_true, y_pred)\n",
    "        elif metrics == 'mae':\n",
    "            return self._mae(y_true, y_pred)\n",
    "        elif metrics == 'mape':\n",
    "            return self._mape(y_true, y_pred)\n",
    "        elif metrics == 'auc':\n",
    "            return self._auc(y_true, y_pred)\n",
    "        elif metrics == 'logloss':\n",
    "            return self._logloss(y_true, y_pred)\n",
    "        elif metrics == 'qwk':\n",
    "            return self._qwk(y_true, y_pred)\n",
    "        else:\n",
    "            raise(NotImplementedError)\n",
    "\n",
    "    # ===============\n",
    "    # RMSE\n",
    "    # ===============\n",
    "    def _rmse(self, y_true, y_pred):\n",
    "        return np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    # ===============\n",
    "    # RMSLE\n",
    "    # ===============\n",
    "    def _rmsle(self, y_true, y_pred):\n",
    "        return np.sqrt(metrics.mean_squared_log_error(y_true, y_pred))\n",
    "\n",
    "    # ===============\n",
    "    # MAE\n",
    "    # ===============\n",
    "    def _mae(self, y_true, y_pred):\n",
    "        return metrics.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    # ===============\n",
    "    # MAPE\n",
    "    # ===============\n",
    "    def _mape(self, y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "    # ===============\n",
    "    # AUC\n",
    "    # ===============\n",
    "    def _auc(self, y_true, y_pred):\n",
    "        return metrics.roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # ===============\n",
    "    # Logloss\n",
    "    # ===============\n",
    "    def _logloss(self, y_true, y_pred):\n",
    "        return metrics.log_loss(y_true, y_pred)\n",
    "\n",
    "    # ===============\n",
    "    # QWK\n",
    "    # ===============\n",
    "    def _qwk(y_true, y_pred):\n",
    "        return metrics.cohen_kappa_score(y_true, y_pred, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, input_features):\n",
    "        super(CustomLinear, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, 200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.fc4 = nn.Linear(50, 25)\n",
    "        self.fc5 = nn.Linear(25, 1)\n",
    "        self.dropout = nn.Dropout2d(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "class NNRegressor:\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.model = None\n",
    "        self.params = params\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x=None, va_y=None, cat_features=None, feval=None):\n",
    "\n",
    "        epochs = self.params['epochs']\n",
    "        batch_size = self.params['batch_size']\n",
    "        lr = self.params['lr']\n",
    "        device = self.params['device']\n",
    "        early_stopping = self.params['early_stopping']\n",
    "        stop_flg = False\n",
    "\n",
    "        validation = va_x is not None\n",
    "        tr_x = torch.tensor(tr_x.values, dtype=torch.float32)\n",
    "        tr_y = torch.tensor(tr_y.values, dtype=torch.float32)\n",
    "        train = torch.utils.data.TensorDataset(tr_x, tr_y)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        if validation:\n",
    "            va_x = torch.tensor(va_x.values, dtype=torch.float32)\n",
    "            va_y = torch.tensor(va_y.values, dtype=torch.float32)\n",
    "            valid = torch.utils.data.TensorDataset(va_x, va_y)\n",
    "            valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = CustomLinear(tr_x.shape[1]).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader), eta_min=1e-5)\n",
    "\n",
    "        best_loss = 1e+10\n",
    "        counter = 0\n",
    "\n",
    "        self.all_train_loss = []\n",
    "        self.all_val_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            if counter == early_stopping:\n",
    "                stop_flg = True\n",
    "                break\n",
    "            else:\n",
    "                model.train()\n",
    "                avg_loss = 0.\n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(x_batch.float())\n",
    "                    loss = criterion(y_pred.float(), y_batch.float())\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "#                     scheduler.step()\n",
    "                    avg_loss += loss.item() / len(train_loader)\n",
    "                    \n",
    "                model.eval()\n",
    "                valid_preds_fold = np.zeros((va_x.size(0)))\n",
    "                avg_val_loss = 0.\n",
    "                    \n",
    "                for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "                    y_pred = model(x_batch.float()).detach()\n",
    "                    avg_val_loss += criterion(y_pred.float(), y_batch.float()).item() / len(valid_loader)\n",
    "                    valid_preds_fold[i * batch_size: (i+1) * batch_size] = y_pred.float().numpy()[:, 0]\n",
    "\n",
    "                self.all_train_loss.append(avg_loss)\n",
    "                self.all_val_loss.append(avg_val_loss)\n",
    "\n",
    "                if best_loss >= avg_val_loss:\n",
    "                    best_loss = avg_val_loss\n",
    "                    best_iter = epoch + 1\n",
    "                    counter = 0\n",
    "                    self.model = model\n",
    "                else:\n",
    "                    counter += 1\n",
    "\n",
    "                print(f\"Epoch  {epoch + 1:02}/{epochs:02}     loss: {avg_loss:.3f}     val_loss: {avg_val_loss:.3f}\")\n",
    "        \n",
    "        if stop_flg:\n",
    "            print(f\"\\nEarly stopping {best_iter:02}     best_val_loss: {best_loss:.3f}\")\n",
    "        else:\n",
    "            print(f\"\\nEpoch  {epoch + 1:02}/{epochs:02}     val_loss: {avg_val_loss:.3f}\")\n",
    "\n",
    "    def predict(self, te_x, cat_features=None):\n",
    "\n",
    "        batch_size = self.params['batch_size']\n",
    "        \n",
    "        test_preds_fold = np.zeros(len(te_x))\n",
    "        x_test_tensor = torch.tensor(te_x.values, dtype=torch.float32)\n",
    "        test = torch.utils.data.TensorDataset(x_test_tensor)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        for i, (x_batch,) in enumerate(test_loader):\n",
    "            y_pred = self.model(x_batch.float()).detach()\n",
    "            test_preds_fold[i * batch_size: (i+1) * batch_size] = y_pred.numpy()[:, 0]\n",
    "        return test_preds_fold\n",
    "\n",
    "    def get_train_log(self):\n",
    "        return self.all_train_loss, self.all_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_x, train_y, test_x, params, folds, model_name=None,\n",
    "                      cat_features=None, feval=None, metrics=None, convert_type='raw'):\n",
    "\n",
    "    unique_fold = np.sort(folds['fold_id'].unique())\n",
    "    if convert_type == 'log':\n",
    "        train_y = np.log1p(train_y)\n",
    "\n",
    "    preds = np.zeros((len(test_x), len(unique_fold)))\n",
    "    oof = np.zeros(len(train_x))\n",
    "    scores = []\n",
    "    models = []\n",
    "    s = Scorer()\n",
    "\n",
    "    for fold_ in unique_fold:\n",
    "        print(f'\\n\\nfold{fold_}')\n",
    "        \n",
    "        if 'ridge' in model_name:\n",
    "            model = RIDGRegression(params)\n",
    "        elif 'knn_clf' in model_name:\n",
    "            model = KNNClassifier(params)\n",
    "        elif 'knn_reg' in model_name:\n",
    "            model = KNNRegressor(params)\n",
    "        elif 'svm_clf' in model_name:\n",
    "            model = SVMClassifier(params)\n",
    "        elif 'svm_reg' in model_name:\n",
    "            model = SVMRegressor(params)\n",
    "        elif 'xgb_clf' in model_name:\n",
    "            model = XGBClassifier(params)\n",
    "        elif 'xgb_reg' in model_name:\n",
    "            model = XGBRegressor(params)\n",
    "        elif 'lgbm_reg' in model_name:\n",
    "            model = LGBRegressor(params)\n",
    "        elif 'lgbm_clf' in model_name:\n",
    "            model = LGBClassifier(params)\n",
    "        elif 'cb_reg' in model_name:\n",
    "            model = CBRegressor(params)\n",
    "        elif 'cb_clf' in model_name:\n",
    "            model = CBClassifier(params)\n",
    "        elif 'nn_reg' in model_name:\n",
    "            model = NNRegressor(params)\n",
    "        # elif 'nn_clf' in model_name:\n",
    "        #     model = NNClassifier(params)\n",
    "        else:\n",
    "            raise(NotImplementedError)\n",
    "\n",
    "        tr_x, va_x = train_x[folds['fold_id'] != fold_], train_x[folds['fold_id'] == fold_]\n",
    "        tr_y, va_y = train_y[folds['fold_id'] != fold_], train_y[folds['fold_id'] == fold_]\n",
    "\n",
    "        model.fit(tr_x, tr_y, va_x, va_y, cat_features=cat_features, feval=feval)\n",
    "\n",
    "        va_pred = model.predict(va_x, cat_features)\n",
    "        oof[va_x.index] = va_pred\n",
    "\n",
    "        if convert_type == 'log':\n",
    "            va_y = np.where(np.expm1(va_y) >= 0, np.expm1(va_y), 0)\n",
    "            va_pred = np.where(np.expm1(va_pred) >= 0, np.expm1(va_pred), 0)\n",
    "            score = s.scorer(metrics, va_y, va_pred)\n",
    "        else:\n",
    "            score = s.scorer(metrics, va_y, va_pred)\n",
    "\n",
    "        scores.append(np.round(score, 3))\n",
    "        print(f'\\nScore: {score}')\n",
    "\n",
    "        pred = model.predict(test_x, cat_features)\n",
    "        if convert_type == 'log':\n",
    "            pred = np.where(np.expm１(pred) >= 0, np.expm1(pred), 0)\n",
    "\n",
    "        preds[:, fold_] = pred\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    if convert_type == 'log':\n",
    "        oof = np.where(np.expm1(oof) >= 0, np.expm1(oof), 0)\n",
    "\n",
    "    print('\\n\\n===================================\\n')\n",
    "    print(f'CV: {np.mean(scores)}')\n",
    "    print('\\n===================================\\n\\n')\n",
    "\n",
    "    return models, preds, oof, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "fold0\n",
      "Epoch  01/100     loss: 160051.060     val_loss: 158754.490\n",
      "Epoch  02/100     loss: 151585.108     val_loss: 132978.638\n",
      "Epoch  03/100     loss: 91198.331     val_loss: 44269.150\n",
      "Epoch  04/100     loss: 40140.103     val_loss: 30000.055\n",
      "Epoch  05/100     loss: 38238.608     val_loss: 29867.515\n",
      "Epoch  06/100     loss: 38420.472     val_loss: 29696.777\n",
      "Epoch  07/100     loss: 37826.248     val_loss: 29602.101\n",
      "Epoch  08/100     loss: 37954.460     val_loss: 29765.046\n",
      "Epoch  09/100     loss: 37481.326     val_loss: 29409.470\n",
      "Epoch  10/100     loss: 37842.836     val_loss: 29592.994\n",
      "Epoch  11/100     loss: 37504.780     val_loss: 29444.454\n",
      "Epoch  12/100     loss: 37361.689     val_loss: 29398.919\n",
      "Epoch  13/100     loss: 37235.583     val_loss: 29553.695\n",
      "Epoch  14/100     loss: 37175.690     val_loss: 29453.840\n",
      "Epoch  15/100     loss: 37293.504     val_loss: 29301.040\n",
      "Epoch  16/100     loss: 37271.210     val_loss: 29321.526\n",
      "Epoch  17/100     loss: 37207.903     val_loss: 29354.762\n",
      "Epoch  18/100     loss: 37205.637     val_loss: 29458.542\n",
      "Epoch  19/100     loss: 37146.039     val_loss: 29367.495\n",
      "Epoch  20/100     loss: 37097.780     val_loss: 29424.370\n",
      "Epoch  21/100     loss: 37120.801     val_loss: 29430.384\n",
      "Epoch  22/100     loss: 37054.305     val_loss: 29367.763\n",
      "Epoch  23/100     loss: 37099.429     val_loss: 29409.510\n",
      "Epoch  24/100     loss: 37262.709     val_loss: 29398.074\n",
      "Epoch  25/100     loss: 37002.592     val_loss: 29203.698\n",
      "Epoch  26/100     loss: 37101.636     val_loss: 29434.682\n",
      "Epoch  27/100     loss: 37075.218     val_loss: 29405.317\n",
      "Epoch  28/100     loss: 37085.233     val_loss: 29463.993\n",
      "Epoch  29/100     loss: 37103.317     val_loss: 29379.156\n",
      "Epoch  30/100     loss: 37019.798     val_loss: 29525.945\n",
      "Epoch  31/100     loss: 36935.450     val_loss: 29330.523\n",
      "Epoch  32/100     loss: 36877.948     val_loss: 29177.957\n",
      "Epoch  33/100     loss: 36956.474     val_loss: 29147.731\n",
      "Epoch  34/100     loss: 36913.661     val_loss: 29489.727\n",
      "Epoch  35/100     loss: 37033.153     val_loss: 29327.416\n",
      "Epoch  36/100     loss: 36987.878     val_loss: 29451.256\n",
      "Epoch  37/100     loss: 36978.783     val_loss: 29548.153\n",
      "Epoch  38/100     loss: 37056.157     val_loss: 29385.711\n",
      "Epoch  39/100     loss: 36963.375     val_loss: 29034.636\n",
      "Epoch  40/100     loss: 36888.115     val_loss: 29170.996\n",
      "Epoch  41/100     loss: 37175.387     val_loss: 29183.753\n",
      "Epoch  42/100     loss: 37065.204     val_loss: 29382.448\n",
      "Epoch  43/100     loss: 36967.668     val_loss: 29456.347\n",
      "Epoch  44/100     loss: 37009.138     val_loss: 29373.499\n",
      "Epoch  45/100     loss: 37039.080     val_loss: 29292.137\n",
      "Epoch  46/100     loss: 36782.708     val_loss: 29371.507\n",
      "Epoch  47/100     loss: 36820.255     val_loss: 29585.921\n",
      "Epoch  48/100     loss: 36928.620     val_loss: 29708.950\n",
      "Epoch  49/100     loss: 37089.174     val_loss: 29197.663\n",
      "Early stopping 39     best_val_loss: 29034.636\n",
      "\n",
      "Score: 132.3779155771291\n",
      "\n",
      "\n",
      "fold1\n",
      "Epoch  01/100     loss: 159118.297     val_loss: 161400.418\n",
      "Epoch  02/100     loss: 150056.339     val_loss: 133948.113\n",
      "Epoch  03/100     loss: 87534.806     val_loss: 42405.253\n",
      "Epoch  04/100     loss: 38172.933     val_loss: 30899.990\n",
      "Epoch  05/100     loss: 36667.768     val_loss: 30474.124\n",
      "Epoch  06/100     loss: 36293.997     val_loss: 30697.026\n",
      "Epoch  07/100     loss: 36319.908     val_loss: 30335.730\n",
      "Epoch  08/100     loss: 35965.018     val_loss: 30459.033\n",
      "Epoch  09/100     loss: 35987.272     val_loss: 30357.064\n",
      "Epoch  10/100     loss: 35775.231     val_loss: 30230.081\n",
      "Epoch  11/100     loss: 35757.107     val_loss: 30377.300\n",
      "Epoch  12/100     loss: 35559.222     val_loss: 30210.948\n",
      "Epoch  13/100     loss: 35695.097     val_loss: 30391.660\n",
      "Epoch  14/100     loss: 35440.355     val_loss: 30336.711\n",
      "Epoch  15/100     loss: 35593.707     val_loss: 30075.009\n",
      "Epoch  16/100     loss: 35397.493     val_loss: 30288.224\n",
      "Epoch  17/100     loss: 35254.234     val_loss: 30194.533\n",
      "Epoch  18/100     loss: 35336.575     val_loss: 30155.959\n",
      "Epoch  19/100     loss: 35413.675     val_loss: 30281.008\n",
      "Epoch  20/100     loss: 35239.556     val_loss: 30075.658\n",
      "Epoch  21/100     loss: 35396.406     val_loss: 30135.126\n",
      "Epoch  22/100     loss: 35538.264     val_loss: 30462.971\n",
      "Epoch  23/100     loss: 35315.704     val_loss: 30350.190\n",
      "Epoch  24/100     loss: 35294.791     val_loss: 30292.942\n",
      "Epoch  25/100     loss: 35315.468     val_loss: 30215.054\n",
      "Early stopping 15     best_val_loss: 30075.009\n",
      "\n",
      "Score: 134.1921381137652\n",
      "\n",
      "\n",
      "fold2\n",
      "Epoch  01/100     loss: 159307.100     val_loss: 157259.170\n",
      "Epoch  02/100     loss: 143586.226     val_loss: 117770.452\n",
      "Epoch  03/100     loss: 73330.365     val_loss: 35715.683\n",
      "Epoch  04/100     loss: 37875.633     val_loss: 30769.389\n",
      "Epoch  05/100     loss: 36965.707     val_loss: 30280.776\n",
      "Epoch  06/100     loss: 36787.082     val_loss: 30325.957\n",
      "Epoch  07/100     loss: 36537.953     val_loss: 30212.439\n",
      "Epoch  08/100     loss: 36300.193     val_loss: 30123.731\n",
      "Epoch  09/100     loss: 36320.740     val_loss: 30116.924\n",
      "Epoch  10/100     loss: 36275.528     val_loss: 30009.595\n",
      "Epoch  11/100     loss: 36058.243     val_loss: 30144.427\n",
      "Epoch  12/100     loss: 36282.773     val_loss: 30073.258\n",
      "Epoch  13/100     loss: 35887.136     val_loss: 30014.091\n",
      "Epoch  14/100     loss: 35905.099     val_loss: 29918.407\n",
      "Epoch  15/100     loss: 35862.178     val_loss: 30351.098\n",
      "Epoch  16/100     loss: 35784.579     val_loss: 30152.729\n",
      "Epoch  17/100     loss: 35678.749     val_loss: 30166.287\n",
      "Epoch  18/100     loss: 35670.574     val_loss: 29884.739\n",
      "Epoch  19/100     loss: 35666.588     val_loss: 30016.987\n",
      "Epoch  20/100     loss: 35784.672     val_loss: 30144.333\n",
      "Epoch  21/100     loss: 35725.743     val_loss: 30003.730\n",
      "Epoch  22/100     loss: 35630.763     val_loss: 29870.448\n",
      "Epoch  23/100     loss: 35619.972     val_loss: 29944.118\n",
      "Epoch  24/100     loss: 35643.165     val_loss: 30012.715\n",
      "Epoch  25/100     loss: 35545.557     val_loss: 30012.116\n",
      "Epoch  26/100     loss: 35574.378     val_loss: 30070.375\n",
      "Epoch  27/100     loss: 35493.978     val_loss: 30146.306\n",
      "Epoch  28/100     loss: 35583.651     val_loss: 30001.839\n",
      "Epoch  29/100     loss: 35588.925     val_loss: 29884.916\n",
      "Epoch  30/100     loss: 35408.009     val_loss: 29972.250\n",
      "Epoch  31/100     loss: 35372.882     val_loss: 30039.992\n",
      "Epoch  32/100     loss: 35278.177     val_loss: 30225.707\n",
      "Early stopping 22     best_val_loss: 29870.448\n",
      "\n",
      "Score: 133.78749226689726\n",
      "\n",
      "\n",
      "fold3\n",
      "Epoch  01/100     loss: 159686.307     val_loss: 158310.823\n",
      "Epoch  02/100     loss: 151980.413     val_loss: 135272.941\n",
      "Epoch  03/100     loss: 94252.471     val_loss: 46178.848\n",
      "Epoch  04/100     loss: 39535.817     val_loss: 30772.956\n",
      "Epoch  05/100     loss: 36881.534     val_loss: 30432.113\n",
      "Epoch  06/100     loss: 36737.261     val_loss: 30120.344\n",
      "Epoch  07/100     loss: 36614.612     val_loss: 30283.990\n",
      "Epoch  08/100     loss: 36330.281     val_loss: 30247.591\n",
      "Epoch  09/100     loss: 36454.078     val_loss: 30197.111\n",
      "Epoch  10/100     loss: 36331.618     val_loss: 30219.617\n",
      "Epoch  11/100     loss: 36139.248     val_loss: 30191.699\n",
      "Epoch  12/100     loss: 36131.586     val_loss: 30109.971\n",
      "Epoch  13/100     loss: 35886.717     val_loss: 30106.688\n",
      "Epoch  14/100     loss: 35907.300     val_loss: 30103.965\n",
      "Epoch  15/100     loss: 35877.961     val_loss: 30190.397\n",
      "Epoch  16/100     loss: 35714.723     val_loss: 30026.894\n",
      "Epoch  17/100     loss: 35632.964     val_loss: 30061.324\n",
      "Epoch  18/100     loss: 35701.551     val_loss: 30299.395\n",
      "Epoch  19/100     loss: 35760.655     val_loss: 30221.945\n",
      "Epoch  20/100     loss: 35525.596     val_loss: 30016.428\n",
      "Epoch  21/100     loss: 35667.667     val_loss: 29979.749\n",
      "Epoch  22/100     loss: 35578.620     val_loss: 30165.779\n",
      "Epoch  23/100     loss: 35867.402     val_loss: 30150.352\n",
      "Epoch  24/100     loss: 35582.248     val_loss: 30100.715\n",
      "Epoch  25/100     loss: 35560.223     val_loss: 30053.527\n",
      "Epoch  26/100     loss: 35595.544     val_loss: 29958.128\n",
      "Epoch  27/100     loss: 35561.921     val_loss: 30158.122\n",
      "Epoch  28/100     loss: 35764.771     val_loss: 29916.171\n",
      "Epoch  29/100     loss: 35561.636     val_loss: 29997.372\n",
      "Epoch  30/100     loss: 35538.670     val_loss: 30138.170\n",
      "Epoch  31/100     loss: 35487.955     val_loss: 30009.914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  32/100     loss: 35701.008     val_loss: 30038.193\n",
      "Epoch  33/100     loss: 35596.749     val_loss: 30167.616\n",
      "Epoch  34/100     loss: 35594.295     val_loss: 30010.970\n",
      "Epoch  35/100     loss: 35600.766     val_loss: 30047.030\n",
      "Epoch  36/100     loss: 35455.726     val_loss: 30041.477\n",
      "Epoch  37/100     loss: 35530.324     val_loss: 30057.065\n",
      "Epoch  38/100     loss: 35582.802     val_loss: 30019.873\n",
      "Early stopping 28     best_val_loss: 29916.171\n",
      "\n",
      "Score: 134.9559492123867\n",
      "\n",
      "\n",
      "fold4\n",
      "Epoch  01/100     loss: 159959.050     val_loss: 156676.313\n",
      "Epoch  02/100     loss: 148113.092     val_loss: 124171.261\n",
      "Epoch  03/100     loss: 80142.068     val_loss: 36480.424\n",
      "Epoch  04/100     loss: 37973.205     val_loss: 30106.932\n",
      "Epoch  05/100     loss: 37238.173     val_loss: 29915.706\n",
      "Epoch  06/100     loss: 36744.065     val_loss: 29724.832\n",
      "Epoch  07/100     loss: 36866.773     val_loss: 29847.241\n",
      "Epoch  08/100     loss: 36656.139     val_loss: 29584.138\n",
      "Epoch  09/100     loss: 36554.954     val_loss: 29723.301\n",
      "Epoch  10/100     loss: 36408.991     val_loss: 29701.890\n",
      "Epoch  11/100     loss: 36296.428     val_loss: 29621.902\n",
      "Epoch  12/100     loss: 36421.266     val_loss: 29531.954\n",
      "Epoch  13/100     loss: 36127.863     val_loss: 29537.627\n",
      "Epoch  14/100     loss: 36212.570     val_loss: 29613.467\n",
      "Epoch  15/100     loss: 36186.500     val_loss: 29592.669\n",
      "Epoch  16/100     loss: 35943.858     val_loss: 29549.301\n",
      "Epoch  17/100     loss: 36140.450     val_loss: 29792.895\n",
      "Epoch  18/100     loss: 36090.851     val_loss: 29515.680\n",
      "Epoch  19/100     loss: 36016.713     val_loss: 29556.956\n",
      "Epoch  20/100     loss: 36047.448     val_loss: 29529.253\n",
      "Epoch  21/100     loss: 35978.652     val_loss: 29658.093\n",
      "Epoch  22/100     loss: 35838.518     val_loss: 29514.879\n",
      "Epoch  23/100     loss: 35854.407     val_loss: 29567.333\n",
      "Epoch  24/100     loss: 35780.265     val_loss: 29551.723\n",
      "Epoch  25/100     loss: 35849.198     val_loss: 29572.586\n",
      "Epoch  26/100     loss: 35938.816     val_loss: 29714.297\n",
      "Epoch  27/100     loss: 35843.715     val_loss: 29644.550\n",
      "Epoch  28/100     loss: 35963.524     val_loss: 29479.792\n",
      "Epoch  29/100     loss: 35816.523     val_loss: 29704.923\n",
      "Epoch  30/100     loss: 35974.647     val_loss: 29761.363\n",
      "Epoch  31/100     loss: 35918.797     val_loss: 29616.806\n",
      "Epoch  32/100     loss: 36009.342     val_loss: 29649.578\n",
      "Epoch  33/100     loss: 35909.793     val_loss: 29542.242\n",
      "Epoch  34/100     loss: 35883.555     val_loss: 29712.614\n",
      "Epoch  35/100     loss: 35881.800     val_loss: 29740.064\n",
      "Epoch  36/100     loss: 35657.449     val_loss: 29592.742\n",
      "Epoch  37/100     loss: 35520.460     val_loss: 29474.190\n",
      "Epoch  38/100     loss: 35506.120     val_loss: 29522.285\n",
      "Epoch  39/100     loss: 35533.684     val_loss: 29444.678\n",
      "Epoch  40/100     loss: 35464.370     val_loss: 29418.170\n",
      "Epoch  41/100     loss: 35399.082     val_loss: 29552.863\n",
      "Epoch  42/100     loss: 35391.350     val_loss: 29698.254\n",
      "Epoch  43/100     loss: 35401.810     val_loss: 29593.242\n",
      "Epoch  44/100     loss: 35451.007     val_loss: 29492.040\n",
      "Epoch  45/100     loss: 35400.702     val_loss: 29397.234\n",
      "Epoch  46/100     loss: 35455.060     val_loss: 29696.683\n",
      "Epoch  47/100     loss: 35268.357     val_loss: 29387.888\n",
      "Epoch  48/100     loss: 35384.665     val_loss: 29641.068\n",
      "Epoch  49/100     loss: 35322.620     val_loss: 29566.303\n",
      "Epoch  50/100     loss: 35414.149     val_loss: 29578.026\n",
      "Epoch  51/100     loss: 35292.594     val_loss: 29446.527\n",
      "Epoch  52/100     loss: 35285.537     val_loss: 29468.294\n",
      "Epoch  53/100     loss: 35357.889     val_loss: 29457.077\n",
      "Epoch  54/100     loss: 35390.980     val_loss: 29958.320\n",
      "Epoch  55/100     loss: 35240.105     val_loss: 29358.901\n",
      "Epoch  56/100     loss: 35395.417     val_loss: 29555.015\n",
      "Epoch  57/100     loss: 35360.238     val_loss: 29428.127\n",
      "Epoch  58/100     loss: 35383.350     val_loss: 29442.103\n",
      "Epoch  59/100     loss: 35282.918     val_loss: 29517.103\n",
      "Epoch  60/100     loss: 35407.335     val_loss: 29474.738\n",
      "Epoch  61/100     loss: 35324.635     val_loss: 29552.696\n",
      "Epoch  62/100     loss: 35326.282     val_loss: 29445.303\n",
      "Epoch  63/100     loss: 35254.257     val_loss: 29403.772\n",
      "Epoch  64/100     loss: 35284.850     val_loss: 29460.721\n",
      "Epoch  65/100     loss: 35333.135     val_loss: 29449.340\n",
      "Early stopping 55     best_val_loss: 29358.901\n",
      "\n",
      "Score: 134.82208961158523\n",
      "\n",
      "\n",
      "===================================\n",
      "\n",
      "CV: 134.027\n",
      "\n",
      "===================================\n",
      "\n",
      "\n",
      "[train and predict] done in 745 s (Total: 12.43 min)\n"
     ]
    }
   ],
   "source": [
    "with t.timer('train and predict'):\n",
    "    models, preds, oof, scores = train_and_predict(train_x, train_y, test_x,\n",
    "                                                   MODEL_PARAMS,\n",
    "                                                   folds,\n",
    "                                                   model_name=MODEL_NAME,\n",
    "                                                   cat_features=SETTINGS_PARAMS['categorical_features'],\n",
    "                                                   feval=None,\n",
    "                                                   metrics=SETTINGS_PARAMS['metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        428.074887\n",
       "1        317.930517\n",
       "2        357.350316\n",
       "3        201.310911\n",
       "4        178.067475\n",
       "            ...    \n",
       "20995    181.735475\n",
       "20996    201.720711\n",
       "20997    364.386736\n",
       "20998    235.686449\n",
       "20999    265.061838\n",
       "Name: salary, Length: 21000, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>341.257874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>343.196167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>334.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>345.766205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>346.556152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>341.738556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>336.555603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>344.639008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>339.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>348.651825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "0      341.257874\n",
       "1      343.196167\n",
       "2      334.102631\n",
       "3      345.766205\n",
       "4      346.556152\n",
       "...           ...\n",
       "20995  341.738556\n",
       "20996  336.555603\n",
       "20997  344.639008\n",
       "20998  339.033600\n",
       "20999  348.651825\n",
       "\n",
       "[21000 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd7b04fad30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a, b = models[1].get_train_log()\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.plot(a, '-o', label='train')\n",
    "plt.plot(b, '-o', label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
